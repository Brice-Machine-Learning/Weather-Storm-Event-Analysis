{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12c553cc-142a-47fc-b770-aaa4972aaaa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_year': 1996,\n",
       " 'include_years': None,\n",
       " 'accept_extensions': ('*.csv', '*.csv.gz'),\n",
       " 'dry_run': False,\n",
       " 'save_interim_as': 'csv',\n",
       " 'project_root': '/home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Project: Storm Events Analysis (NOAA)\n",
    "# Notebook: 00_data_download.ipynb\n",
    "# Goal: Determine data shapes and column IDs, plan for combining data, implement data compiling.\n",
    "# Author: Brice Nelson\n",
    "# Date: 2025-09-05\n",
    "\n",
    "from pathlib import Path\n",
    "import re\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "# --- Display & Repro ---\n",
    "plt.rcParams[\"figure.dpi\"] = 130\n",
    "pd.set_option(\"display.max_rows\", 25)\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# --- Paths ---\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == \"notebooks\" else Path.cwd()\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_INTERIM = PROJECT_ROOT / \"data\" / \"interim\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "REPORT_FIGS = PROJECT_ROOT / \"reports\" / \"figures\"\n",
    "REPORT_TABLES = PROJECT_ROOT / \"reports\" / \"tables\"\n",
    "\n",
    "for p in [\n",
    "    DATA_INTERIM / \"details\",\n",
    "    DATA_INTERIM / \"locations\",\n",
    "    DATA_INTERIM / \"fatalities\",\n",
    "    REPORT_FIGS,\n",
    "    REPORT_TABLES,\n",
    "    DATA_INTERIM / \"_logs\",\n",
    "]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Notebook knobs (tweak here) ---\n",
    "MIN_YEAR = 1996                    # only process files from this year onward\n",
    "INCLUDE_YEARS = None               # e.g., set to range(1996, 2021) or a set like {2018, 2019, 2020}; None = auto\n",
    "ACCEPT_EXTENSIONS = (\"*.csv\", \"*.csv.gz\")\n",
    "DRY_RUN = False                    # True = preview without writing to data/interim\n",
    "SAVE_INTERIM_AS = \"csv\"            # \"csv\" (default) or \"parquet\" (later, if desired)\n",
    "\n",
    "# --- Filename/type detection (no functions here) ---\n",
    "# Handles names like: StormEvents_details-ftp_v1.0_d2020_c20210108.csv(.gz)\n",
    "YEAR_DIR_RE = re.compile(r\"^(?:19|20)\\d{2}$\")  # folder named 1996, 2020, etc.\n",
    "YEAR_IN_NAME_D_PATTERN = re.compile(r\"d((?:19|20)\\d{2})\", re.I)  # ..._d2020_...\n",
    "YEAR_IN_NAME_ANY_PATTERN = re.compile(r\"(?:19|20)\\d{2}\")\n",
    "\n",
    "TYPE_PATTERNS = {\n",
    "    \"details\":    re.compile(r\"(?:^|[_\\W])detail[s]?(?:[_\\W]|$)\", re.I),\n",
    "    \"locations\":  re.compile(r\"(?:^|[_\\W])loc(?:ation|ations)?(?:[_\\W]|$)\", re.I),\n",
    "    \"fatalities\": re.compile(r\"(?:^|[_\\W])fatal(?:ity|ities)?(?:[_\\W]|$)\", re.I),\n",
    "}\n",
    "\n",
    "# If your files use different tokens, add them here (e.g., {\"details\": r\"\\bDET\\b\"})\n",
    "EXTRA_TYPE_TOKENS = {\n",
    "    # \"details\": re.compile(r\"\\bDET\\b\", re.I),\n",
    "    # \"locations\": re.compile(r\"\\bLOC\\b\", re.I),\n",
    "    # \"fatalities\": re.compile(r\"\\bFAT\\b\", re.I),\n",
    "}\n",
    "\n",
    "# --- Column normalization hints (used in later cells) ---\n",
    "LOWERCASE_COLS = True\n",
    "EVENT_ID_CANDIDATES = (\n",
    "    \"event_id\", \"event id\", \"eventid\", \"EVENT_ID\", \"Event_ID\", \"Event Id\"\n",
    ")\n",
    "\n",
    "# Pandas read options (you can tweak for speed/memory)\n",
    "READ_KWARGS = dict(low_memory=False)  # compression will be inferred for .csv.gz\n",
    "\n",
    "# Output mapping (used by later cells)\n",
    "OUTPUT_SUBDIRS = {\n",
    "    \"details\": DATA_INTERIM / \"details\",\n",
    "    \"locations\": DATA_INTERIM / \"locations\",\n",
    "    \"fatalities\": DATA_INTERIM / \"fatalities\",\n",
    "}\n",
    "\n",
    "# Small metadata dict for logging\n",
    "RUN_META = {\n",
    "    \"min_year\": MIN_YEAR,\n",
    "    \"include_years\": (list(INCLUDE_YEARS) if INCLUDE_YEARS is not None else None),\n",
    "    \"accept_extensions\": ACCEPT_EXTENSIONS,\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"save_interim_as\": SAVE_INTERIM_AS,\n",
    "    \"project_root\": str(PROJECT_ROOT),\n",
    "}\n",
    "RUN_META\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5085cd90-88b6-4dc7-b1a8-1477798a6c00",
   "metadata": {},
   "source": [
    "## Determine shape of files and information in files for planning of data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe38cc90-0382-4662-b8d3-45a5199d06dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (471, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 471 entries, 0 to 470\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   FAT_YEARMONTH      471 non-null    int64  \n",
      " 1   FAT_DAY            471 non-null    int64  \n",
      " 2   FAT_TIME           471 non-null    int64  \n",
      " 3   FATALITY_ID        471 non-null    int64  \n",
      " 4   EVENT_ID           471 non-null    int64  \n",
      " 5   FATALITY_TYPE      471 non-null    object \n",
      " 6   FATALITY_DATE      471 non-null    object \n",
      " 7   FATALITY_AGE       400 non-null    float64\n",
      " 8   FATALITY_SEX       438 non-null    object \n",
      " 9   FATALITY_LOCATION  471 non-null    object \n",
      " 10  EVENT_YEARMONTH    471 non-null    int64  \n",
      "dtypes: float64(1), int64(6), object(4)\n",
      "memory usage: 40.6+ KB\n",
      "info: None\n"
     ]
    }
   ],
   "source": [
    "storm_fatalities_2020 = pd.read_csv(\"../data/raw/archive/StormEvents_fatalities-ftp_v1.0_d2020_c20201216.csv\")\n",
    "print(f'Shape: {storm_fatalities_2020.shape}')\n",
    "print(f'info: {storm_fatalities_2020.info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd813ac-e11d-4fca-9c93-9fd832ff2357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (50317, 51)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50317 entries, 0 to 50316\n",
      "Data columns (total 51 columns):\n",
      " #   Column              Non-Null Count  Dtype  \n",
      "---  ------              --------------  -----  \n",
      " 0   BEGIN_YEARMONTH     50317 non-null  int64  \n",
      " 1   BEGIN_DAY           50317 non-null  int64  \n",
      " 2   BEGIN_TIME          50317 non-null  int64  \n",
      " 3   END_YEARMONTH       50317 non-null  int64  \n",
      " 4   END_DAY             50317 non-null  int64  \n",
      " 5   END_TIME            50317 non-null  int64  \n",
      " 6   EPISODE_ID          50317 non-null  int64  \n",
      " 7   EVENT_ID            50317 non-null  int64  \n",
      " 8   STATE               50317 non-null  object \n",
      " 9   STATE_FIPS          50317 non-null  int64  \n",
      " 10  YEAR                50317 non-null  int64  \n",
      " 11  MONTH_NAME          50317 non-null  object \n",
      " 12  EVENT_TYPE          50317 non-null  object \n",
      " 13  CZ_TYPE             50317 non-null  object \n",
      " 14  CZ_FIPS             50317 non-null  int64  \n",
      " 15  CZ_NAME             50317 non-null  object \n",
      " 16  WFO                 50317 non-null  object \n",
      " 17  BEGIN_DATE_TIME     50317 non-null  object \n",
      " 18  CZ_TIMEZONE         50317 non-null  object \n",
      " 19  END_DATE_TIME       50317 non-null  object \n",
      " 20  INJURIES_DIRECT     50317 non-null  int64  \n",
      " 21  INJURIES_INDIRECT   50317 non-null  int64  \n",
      " 22  DEATHS_DIRECT       50317 non-null  int64  \n",
      " 23  DEATHS_INDIRECT     50317 non-null  int64  \n",
      " 24  DAMAGE_PROPERTY     39824 non-null  object \n",
      " 25  DAMAGE_CROPS        40641 non-null  object \n",
      " 26  SOURCE              50317 non-null  object \n",
      " 27  MAGNITUDE           31312 non-null  float64\n",
      " 28  MAGNITUDE_TYPE      23758 non-null  object \n",
      " 29  FLOOD_CAUSE         5568 non-null   object \n",
      " 30  CATEGORY            37 non-null     float64\n",
      " 31  TOR_F_SCALE         1164 non-null   object \n",
      " 32  TOR_LENGTH          1164 non-null   float64\n",
      " 33  TOR_WIDTH           1164 non-null   float64\n",
      " 34  TOR_OTHER_WFO       163 non-null    object \n",
      " 35  TOR_OTHER_CZ_STATE  163 non-null    object \n",
      " 36  TOR_OTHER_CZ_FIPS   163 non-null    float64\n",
      " 37  TOR_OTHER_CZ_NAME   163 non-null    object \n",
      " 38  BEGIN_RANGE         36837 non-null  float64\n",
      " 39  BEGIN_AZIMUTH       36837 non-null  object \n",
      " 40  BEGIN_LOCATION      36837 non-null  object \n",
      " 41  END_RANGE           36837 non-null  float64\n",
      " 42  END_AZIMUTH         36837 non-null  object \n",
      " 43  END_LOCATION        36837 non-null  object \n",
      " 44  BEGIN_LAT           36837 non-null  float64\n",
      " 45  BEGIN_LON           36837 non-null  float64\n",
      " 46  END_LAT             36837 non-null  float64\n",
      " 47  END_LON             36837 non-null  float64\n",
      " 48  EPISODE_NARRATIVE   50317 non-null  object \n",
      " 49  EVENT_NARRATIVE     40587 non-null  object \n",
      " 50  DATA_SOURCE         50317 non-null  object \n",
      "dtypes: float64(11), int64(15), object(25)\n",
      "memory usage: 19.6+ MB\n",
      "info: None\n"
     ]
    }
   ],
   "source": [
    "storm_details_2020 = pd.read_csv(\"../data/raw/archive/StormEvents_details-ftp_v1.0_d2020_c20201216.csv\")\n",
    "print(f'Shape: {storm_details_2020.shape}')\n",
    "print(f'info: {storm_details_2020.info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55949b56-84d9-4e32-9e81-41576774e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (48968, 11)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48968 entries, 0 to 48967\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   YEARMONTH       48968 non-null  int64  \n",
      " 1   EPISODE_ID      48968 non-null  int64  \n",
      " 2   EVENT_ID        48968 non-null  int64  \n",
      " 3   LOCATION_INDEX  48968 non-null  int64  \n",
      " 4   RANGE           48968 non-null  float64\n",
      " 5   AZIMUTH         48968 non-null  object \n",
      " 6   LOCATION        48968 non-null  object \n",
      " 7   LATITUDE        48968 non-null  float64\n",
      " 8   LONGITUDE       48968 non-null  float64\n",
      " 9   LAT2            48968 non-null  int64  \n",
      " 10  LON2            48968 non-null  int64  \n",
      "dtypes: float64(3), int64(6), object(2)\n",
      "memory usage: 4.1+ MB\n",
      "info: None\n"
     ]
    }
   ],
   "source": [
    "storm_locations_2020 = pd.read_csv(\"../data/raw/archive/StormEvents_locations-ftp_v1.0_d2020_c20201216.csv\")\n",
    "print(f'Shape: {storm_locations_2020.shape}')\n",
    "print(f'info: {storm_locations_2020.info()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68ad3c29-8c73-4210-b05b-b3a487b43410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1950_c20170120.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1951_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1952_c20170619.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1953_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1954_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1955_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1956_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1957_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1958_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1959_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1960_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1961_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1962_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1963_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1964_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1965_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1966_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1967_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1968_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1969_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1970_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1971_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1972_c20181029.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1973_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1974_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1975_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1976_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1977_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1978_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1979_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1980_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1981_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1982_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1983_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1984_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1985_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1986_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1987_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1988_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1989_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1990_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1991_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1992_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1993_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1994_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_details-ftp_v1.0_d1995_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1950_c20170120.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1951_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1952_c20170619.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1953_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1954_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1955_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1956_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1957_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1958_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1959_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1960_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1961_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1962_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1963_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1964_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1965_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1966_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1967_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1968_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1969_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1970_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1971_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1972_c20181029.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1973_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1974_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1975_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1976_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1977_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1978_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1979_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1980_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1981_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1982_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1983_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1984_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1985_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1986_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1987_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1988_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1989_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1990_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1991_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1992_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1993_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1994_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_fatalities-ftp_v1.0_d1995_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1950_c20170120.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1951_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1952_c20170619.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1953_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1954_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1955_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1956_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1957_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1958_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1959_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1960_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1961_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1962_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1963_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1964_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1965_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1966_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1967_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1968_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1969_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1970_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1971_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1972_c20181029.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1973_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1974_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1975_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1976_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1977_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1978_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1979_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1980_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1981_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1982_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1983_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1984_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1985_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1986_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1987_c20160223.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1988_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1989_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1990_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1991_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1992_c20170717.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1993_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1994_c20190920.csv\n",
      "[skip] /home/bnelson_regex/projects/machine_learning_projects/weather_storm_events_predict/data/raw/archive/StormEvents_locations-ftp_v1.0_d1995_c20190920.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>type</th>\n",
       "      <th>rows</th>\n",
       "      <th>saved_as</th>\n",
       "      <th>saved_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996</td>\n",
       "      <td>details</td>\n",
       "      <td>48561</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>1996</td>\n",
       "      <td>details</td>\n",
       "      <td>48561</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1997</td>\n",
       "      <td>details</td>\n",
       "      <td>41991</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>1997</td>\n",
       "      <td>details</td>\n",
       "      <td>41991</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1998</td>\n",
       "      <td>details</td>\n",
       "      <td>50973</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>2018</td>\n",
       "      <td>locations</td>\n",
       "      <td>47529</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2019</td>\n",
       "      <td>locations</td>\n",
       "      <td>52677</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>2019</td>\n",
       "      <td>locations</td>\n",
       "      <td>52677</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2020</td>\n",
       "      <td>locations</td>\n",
       "      <td>48968</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>2020</td>\n",
       "      <td>locations</td>\n",
       "      <td>48968</td>\n",
       "      <td>csv</td>\n",
       "      <td>/home/bnelson_regex/projects/machine_learning_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year       type   rows saved_as  \\\n",
       "0    1996    details  48561      csv   \n",
       "139  1996    details  48561      csv   \n",
       "3    1997    details  41991      csv   \n",
       "140  1997    details  41991      csv   \n",
       "6    1998    details  50973      csv   \n",
       "..    ...        ...    ...      ...   \n",
       "136  2018  locations  47529      csv   \n",
       "71   2019  locations  52677      csv   \n",
       "137  2019  locations  52677      csv   \n",
       "74   2020  locations  48968      csv   \n",
       "138  2020  locations  48968      csv   \n",
       "\n",
       "                                            saved_path  \n",
       "0    /home/bnelson_regex/projects/machine_learning_...  \n",
       "139  /home/bnelson_regex/projects/machine_learning_...  \n",
       "3    /home/bnelson_regex/projects/machine_learning_...  \n",
       "140  /home/bnelson_regex/projects/machine_learning_...  \n",
       "6    /home/bnelson_regex/projects/machine_learning_...  \n",
       "..                                                 ...  \n",
       "136  /home/bnelson_regex/projects/machine_learning_...  \n",
       "71   /home/bnelson_regex/projects/machine_learning_...  \n",
       "137  /home/bnelson_regex/projects/machine_learning_...  \n",
       "74   /home/bnelson_regex/projects/machine_learning_...  \n",
       "138  /home/bnelson_regex/projects/machine_learning_...  \n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def infer_year_from_path(fp: Path) -> int | None:\n",
    "    # Prefer parent folder like data/raw/2020/...\n",
    "    parent = fp.parent.name\n",
    "    if YEAR_DIR_RE.fullmatch(parent):\n",
    "        return int(parent)\n",
    "    # Fallbacks: NOAA pattern like ..._d2020_..., then any 4-digit year\n",
    "    m = YEAR_IN_NAME_D_PATTERN.search(fp.name)\n",
    "    if m:\n",
    "        return int(m.group(1))\n",
    "    m = YEAR_IN_NAME_ANY_PATTERN.search(fp.name)\n",
    "    return int(m.group(0)) if m else None\n",
    "\n",
    "def infer_type_from_name(name: str) -> str | None:\n",
    "    n = name.lower()\n",
    "    for t, pat in TYPE_PATTERNS.items():\n",
    "        if pat.search(n):\n",
    "            return t\n",
    "    for t, pat in EXTRA_TYPE_TOKENS.items():\n",
    "        if pat.search(n):\n",
    "            return t\n",
    "    # simple substring fallbacks\n",
    "    if \"detail\" in n: return \"details\"\n",
    "    if \"fatal\" in n:  return \"fatalities\"\n",
    "    if \"location\" in n or \"locat\" in n: return \"locations\"\n",
    "    return None\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    # normalize event_id (covers 'event id', 'eventid', etc.)\n",
    "    if \"event_id\" not in df.columns:\n",
    "        for c in df.columns:\n",
    "            if re.sub(r\"[\\s_]+\", \"\", c) == \"eventid\":\n",
    "                df.rename(columns={c: \"event_id\"}, inplace=True)\n",
    "                break\n",
    "    if \"event_id\" in df.columns:\n",
    "        df[\"event_id\"] = pd.to_numeric(df[\"event_id\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    return df\n",
    "\n",
    "# ---- Main ingest: walk all CSV/CSV.GZ under data/raw/<year>/* ----\n",
    "index_rows = []\n",
    "min_year = globals().get(\"MIN_YEAR\", 1996)\n",
    "include_years = globals().get(\"INCLUDE_YEARS\", None)\n",
    "accept_exts = globals().get(\"ACCEPT_EXTENSIONS\", (\"*.csv\", \"*.csv.gz\"))\n",
    "dry_run = bool(globals().get(\"DRY_RUN\", False))\n",
    "save_as = str(globals().get(\"SAVE_INTERIM_AS\", \"csv\")).lower()\n",
    "read_kwargs = globals().get(\"READ_KWARGS\", dict(low_memory=False))\n",
    "\n",
    "# Collect files by extension(s)\n",
    "files = []\n",
    "for pat in accept_exts:\n",
    "    files.extend(DATA_RAW.rglob(pat))\n",
    "files = sorted(files)\n",
    "\n",
    "for fp in files:\n",
    "    year = infer_year_from_path(fp)\n",
    "    ftype = infer_type_from_name(fp.name)\n",
    "\n",
    "    # Year/type filters\n",
    "    if year is None or year < min_year or ftype is None:\n",
    "        print(f\"[skip] {fp}\")\n",
    "        continue\n",
    "    if include_years is not None and year not in include_years:\n",
    "        print(f\"[skip: not in INCLUDE_YEARS] {fp}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(fp, **read_kwargs)  # compression inferred for .gz\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] reading {fp}: {e}\")\n",
    "        continue\n",
    "\n",
    "    df = standardize_columns(df)\n",
    "    df[\"year\"] = year\n",
    "    df[\"source_filename\"] = fp.name\n",
    "\n",
    "    out_dir = OUTPUT_SUBDIRS.get(ftype, DATA_INTERIM / ftype)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if dry_run:\n",
    "        print(f\"[dry-run] Would save ({len(df)} rows) -> {out_dir / f'{ftype}_{year}.{save_as}'}\")\n",
    "    else:\n",
    "        if save_as == \"parquet\":\n",
    "            try:\n",
    "                df.to_parquet(out_dir / f\"{ftype}_{year}.parquet\", index=False)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Parquet failed ({e}); falling back to CSV for {fp.name}\")\n",
    "                df.to_csv(out_dir / f\"{ftype}_{year}.csv\", index=False)\n",
    "                save_ext = \"csv\"\n",
    "            else:\n",
    "                save_ext = \"parquet\"\n",
    "        else:\n",
    "            df.to_csv(out_dir / f\"{ftype}_{year}.csv\", index=False)\n",
    "            save_ext = \"csv\"\n",
    "\n",
    "        index_rows.append({\n",
    "            \"year\": year,\n",
    "            \"type\": ftype,\n",
    "            \"rows\": int(len(df)),\n",
    "            \"saved_as\": save_ext,\n",
    "            \"saved_path\": str(out_dir / f\"{ftype}_{year}.{save_ext}\")\n",
    "        })\n",
    "\n",
    "# Quick summary + persistent log\n",
    "summary = pd.DataFrame(index_rows).sort_values([\"type\", \"year\"]) if index_rows else pd.DataFrame()\n",
    "display(summary)\n",
    "if not summary.empty:\n",
    "    log_dir = DATA_INTERIM / \"_logs\"\n",
    "    log_dir.mkdir(exist_ok=True)\n",
    "    summary.to_csv(log_dir / \"ingest_summary.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241e32c6-7361-4fbd-8cc5-802a335a6a64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
